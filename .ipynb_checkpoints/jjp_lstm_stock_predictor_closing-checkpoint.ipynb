{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Stock Predictor Using Closing Prices\n",
    "\n",
    "In this notebook, you will build and train a custom LSTM RNN that uses a 10 day window of Bitcoin closing prices to predict the 11th day closing price. \n",
    "\n",
    "You will need to:\n",
    "\n",
    "1. Prepare the data for training and testing\n",
    "2. Build and train a custom LSTM RNN\n",
    "3. Evaluate the performance of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "In this section, you will need to prepare the training and testing data for the model. The model will use a rolling 10 day window to predict the 11th day closing price.\n",
    "\n",
    "You will need to:\n",
    "1. Use the `window_data` function to generate the X and y values for the model.\n",
    "2. Split the data into 70% training and 30% testing\n",
    "3. Apply the MinMaxScaler to the X and y values\n",
    "4. Reshape the X_train and X_test data for the model. Note: The required input format for the LSTM is:\n",
    "\n",
    "```python\n",
    "reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed for reproducibility\n",
    "# Note: This is for the homework solution, but it is good practice to comment this out and run multiple experiments to evaluate your model\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import random\n",
    "random.set_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MMM</th>\n",
       "      <th>ABT</th>\n",
       "      <th>ABBV</th>\n",
       "      <th>ABMD</th>\n",
       "      <th>ACN</th>\n",
       "      <th>ATVI</th>\n",
       "      <th>ADBE</th>\n",
       "      <th>AMD</th>\n",
       "      <th>AAP</th>\n",
       "      <th>AES</th>\n",
       "      <th>...</th>\n",
       "      <th>WLTW</th>\n",
       "      <th>WYNN</th>\n",
       "      <th>XEL</th>\n",
       "      <th>XRX</th>\n",
       "      <th>XLNX</th>\n",
       "      <th>XYL</th>\n",
       "      <th>YUM</th>\n",
       "      <th>ZBH</th>\n",
       "      <th>ZION</th>\n",
       "      <th>ZTS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2019-03-01 00:00:00+00:00</td>\n",
       "      <td>207.49</td>\n",
       "      <td>78.59</td>\n",
       "      <td>80.10</td>\n",
       "      <td>348.06</td>\n",
       "      <td>164.20</td>\n",
       "      <td>42.84</td>\n",
       "      <td>264.01</td>\n",
       "      <td>23.68</td>\n",
       "      <td>160.62</td>\n",
       "      <td>17.50</td>\n",
       "      <td>...</td>\n",
       "      <td>172.56</td>\n",
       "      <td>125.12</td>\n",
       "      <td>54.92</td>\n",
       "      <td>31.29</td>\n",
       "      <td>126.64</td>\n",
       "      <td>76.37</td>\n",
       "      <td>95.37</td>\n",
       "      <td>125.41</td>\n",
       "      <td>51.17</td>\n",
       "      <td>95.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-03-04 00:00:00+00:00</td>\n",
       "      <td>206.86</td>\n",
       "      <td>78.62</td>\n",
       "      <td>79.46</td>\n",
       "      <td>338.20</td>\n",
       "      <td>162.82</td>\n",
       "      <td>41.79</td>\n",
       "      <td>258.16</td>\n",
       "      <td>23.37</td>\n",
       "      <td>158.40</td>\n",
       "      <td>17.12</td>\n",
       "      <td>...</td>\n",
       "      <td>173.05</td>\n",
       "      <td>125.57</td>\n",
       "      <td>55.08</td>\n",
       "      <td>31.36</td>\n",
       "      <td>122.81</td>\n",
       "      <td>76.25</td>\n",
       "      <td>95.14</td>\n",
       "      <td>124.43</td>\n",
       "      <td>50.86</td>\n",
       "      <td>95.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-03-05 00:00:00+00:00</td>\n",
       "      <td>203.84</td>\n",
       "      <td>78.22</td>\n",
       "      <td>78.91</td>\n",
       "      <td>333.21</td>\n",
       "      <td>163.67</td>\n",
       "      <td>41.88</td>\n",
       "      <td>257.38</td>\n",
       "      <td>23.50</td>\n",
       "      <td>157.31</td>\n",
       "      <td>17.42</td>\n",
       "      <td>...</td>\n",
       "      <td>182.04</td>\n",
       "      <td>124.14</td>\n",
       "      <td>54.90</td>\n",
       "      <td>31.09</td>\n",
       "      <td>122.95</td>\n",
       "      <td>76.19</td>\n",
       "      <td>95.93</td>\n",
       "      <td>124.11</td>\n",
       "      <td>50.79</td>\n",
       "      <td>95.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-03-06 00:00:00+00:00</td>\n",
       "      <td>202.80</td>\n",
       "      <td>77.70</td>\n",
       "      <td>78.09</td>\n",
       "      <td>317.85</td>\n",
       "      <td>162.39</td>\n",
       "      <td>42.56</td>\n",
       "      <td>256.40</td>\n",
       "      <td>22.41</td>\n",
       "      <td>157.97</td>\n",
       "      <td>17.58</td>\n",
       "      <td>...</td>\n",
       "      <td>170.88</td>\n",
       "      <td>123.55</td>\n",
       "      <td>55.09</td>\n",
       "      <td>30.85</td>\n",
       "      <td>121.36</td>\n",
       "      <td>75.85</td>\n",
       "      <td>96.60</td>\n",
       "      <td>123.24</td>\n",
       "      <td>49.67</td>\n",
       "      <td>94.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-03-07 00:00:00+00:00</td>\n",
       "      <td>199.86</td>\n",
       "      <td>76.80</td>\n",
       "      <td>78.24</td>\n",
       "      <td>317.42</td>\n",
       "      <td>161.85</td>\n",
       "      <td>41.42</td>\n",
       "      <td>255.46</td>\n",
       "      <td>22.08</td>\n",
       "      <td>154.13</td>\n",
       "      <td>17.67</td>\n",
       "      <td>...</td>\n",
       "      <td>170.20</td>\n",
       "      <td>118.67</td>\n",
       "      <td>55.52</td>\n",
       "      <td>30.24</td>\n",
       "      <td>119.91</td>\n",
       "      <td>75.32</td>\n",
       "      <td>96.54</td>\n",
       "      <td>121.39</td>\n",
       "      <td>48.71</td>\n",
       "      <td>92.55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 503 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              MMM    ABT   ABBV    ABMD     ACN   ATVI  \\\n",
       "date                                                                     \n",
       "2019-03-01 00:00:00+00:00  207.49  78.59  80.10  348.06  164.20  42.84   \n",
       "2019-03-04 00:00:00+00:00  206.86  78.62  79.46  338.20  162.82  41.79   \n",
       "2019-03-05 00:00:00+00:00  203.84  78.22  78.91  333.21  163.67  41.88   \n",
       "2019-03-06 00:00:00+00:00  202.80  77.70  78.09  317.85  162.39  42.56   \n",
       "2019-03-07 00:00:00+00:00  199.86  76.80  78.24  317.42  161.85  41.42   \n",
       "\n",
       "                             ADBE    AMD     AAP    AES  ...    WLTW    WYNN  \\\n",
       "date                                                     ...                   \n",
       "2019-03-01 00:00:00+00:00  264.01  23.68  160.62  17.50  ...  172.56  125.12   \n",
       "2019-03-04 00:00:00+00:00  258.16  23.37  158.40  17.12  ...  173.05  125.57   \n",
       "2019-03-05 00:00:00+00:00  257.38  23.50  157.31  17.42  ...  182.04  124.14   \n",
       "2019-03-06 00:00:00+00:00  256.40  22.41  157.97  17.58  ...  170.88  123.55   \n",
       "2019-03-07 00:00:00+00:00  255.46  22.08  154.13  17.67  ...  170.20  118.67   \n",
       "\n",
       "                             XEL    XRX    XLNX    XYL    YUM     ZBH   ZION  \\\n",
       "date                                                                           \n",
       "2019-03-01 00:00:00+00:00  54.92  31.29  126.64  76.37  95.37  125.41  51.17   \n",
       "2019-03-04 00:00:00+00:00  55.08  31.36  122.81  76.25  95.14  124.43  50.86   \n",
       "2019-03-05 00:00:00+00:00  54.90  31.09  122.95  76.19  95.93  124.11  50.79   \n",
       "2019-03-06 00:00:00+00:00  55.09  30.85  121.36  75.85  96.60  123.24  49.67   \n",
       "2019-03-07 00:00:00+00:00  55.52  30.24  119.91  75.32  96.54  121.39  48.71   \n",
       "\n",
       "                             ZTS  \n",
       "date                              \n",
       "2019-03-01 00:00:00+00:00  95.75  \n",
       "2019-03-04 00:00:00+00:00  95.87  \n",
       "2019-03-05 00:00:00+00:00  95.78  \n",
       "2019-03-06 00:00:00+00:00  94.17  \n",
       "2019-03-07 00:00:00+00:00  92.55  \n",
       "\n",
       "[5 rows x 503 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the stock prices\n",
    "df = pd.read_csv('sp500_close.csv', index_col=\"date\", infer_datetime_format=True, parse_dates=True)\n",
    "#df = df[\"chosen stocks\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function accepts the column number for the features (X) and the target (y)\n",
    "# It chunks the data up with a rolling window of Xt-n to predict Xt\n",
    "# It returns a numpy array of X any y\n",
    "def window_data(df, window, feature_col_number, target_col_number):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(df) - window):\n",
    "        features = df.iloc[i:(i + window), feature_col_number]\n",
    "        target = df.iloc[(i + window), target_col_number]\n",
    "        X.append(features)\n",
    "        y.append(target)\n",
    "    return np.array(X), np.array(y).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Closing Prices using a 10 day window of previous closing prices\n",
    "# Try a window size anywhere from 1 to 10 and see how the model performance changes\n",
    "#jjp - need to add a for loop \n",
    "\n",
    "# Creating the features (X) and target (y) data using the window_data() function.\n",
    "window_size = 1\n",
    "\n",
    "# Column index 1 is the `Close` column\n",
    "feature_column = 5\n",
    "target_column = 5\n",
    "X, y = window_data(df, window_size, feature_column, target_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 70% of the data for training and the remaineder for testing\n",
    "# YOUR CODE HERE!\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "split = int(0.7 *len(X))\n",
    "X_train_rnn = X[: split -1]\n",
    "X_test = X[split:]\n",
    "y_train_rnn = y[: split -1]\n",
    "y_test = y[split:]\n",
    "\n",
    "X_train_rnn, X_val_rnn, y_train_rnn, y_val_rnn = train_test_split(X_train_rnn, y_train_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use MinMaxScaler to scale the data between 0 and 1. \n",
    "# YOUR CODE HERE!\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X)\n",
    "X_train_rnn =scaler.transform(X_train_rnn)\n",
    "X_val_rnn =scaler.transform(X_val_rnn)\n",
    "X_test = scaler.transform(X_test)\n",
    "scaler.fit(y)\n",
    "y_train_rnn = scaler.transform(y_train_rnn)\n",
    "y_val_rnn = scaler.transform(y_val_rnn)\n",
    "y_test =scaler.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train sample values:\n",
      "[[[0.08845316]]\n",
      "\n",
      " [[0.2496732 ]]\n",
      "\n",
      " [[0.23137255]]\n",
      "\n",
      " [[0.15511983]]\n",
      "\n",
      " [[0.17864924]]] \n",
      "\n",
      "X_val_rnn sample values:\n",
      "[[[0.27189542]]\n",
      "\n",
      " [[0.29237473]]\n",
      "\n",
      " [[0.18300654]]\n",
      "\n",
      " [[0.2       ]]\n",
      "\n",
      " [[0.25577342]]] \n",
      "\n",
      "X_test sample values:\n",
      "[[[0.57211329]]\n",
      "\n",
      " [[0.56122004]]\n",
      "\n",
      " [[0.48366013]]\n",
      "\n",
      " [[0.45882353]]\n",
      "\n",
      " [[0.47538126]]]\n"
     ]
    }
   ],
   "source": [
    "# Reshape the features for the model\n",
    "# YOUR CODE HERE!\n",
    "X_train_rnn = X_train_rnn.reshape((X_train_rnn.shape[0], X_train_rnn.shape[1], 1))\n",
    "X_val_rnn = X_val_rnn.reshape((X_val_rnn.shape[0], X_val_rnn.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "print (f\"X_train sample values:\\n{X_train_rnn[:5]} \\n\")\n",
    "print (f\"X_val_rnn sample values:\\n{X_val_rnn[:5]} \\n\")\n",
    "print (f\"X_test sample values:\\n{X_test[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Train the LSTM RNN\n",
    "\n",
    "In this section, you will design a custom LSTM RNN and fit (train) it using the training data.\n",
    "\n",
    "You will need to:\n",
    "1. Define the model architecture\n",
    "2. Compile the model\n",
    "3. Fit the model to the training data\n",
    "\n",
    "### Hints:\n",
    "You will want to use the same model architecture and random seed for both notebooks. This is necessary to accurately compare the performance of the FNG model vs the closing price model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    }
   ],
   "source": [
    "# Build the LSTM model. \n",
    "# The return sequences need to be set to True if you are adding additional LSTM layers, but \n",
    "# You don't have to do this for the final layer. \n",
    "# YOUR CODE HERE!\n",
    "model = Sequential()\n",
    "\n",
    "number_units = 50\n",
    "dropout_fraction = 0.7\n",
    "\n",
    "#first layer:\n",
    "model.add(LSTM(\n",
    "    units=number_units,\n",
    "    return_sequences=True,\n",
    "    input_shape=(X_train_rnn.shape[1], 1))\n",
    "    )\n",
    "model.add(Dropout(dropout_fraction))\n",
    "#second layer\n",
    "model.add(LSTM(units=number_units, return_sequences=True))\n",
    "model.add(Dropout(dropout_fraction))\n",
    "#third layer\n",
    "model.add(LSTM(units=number_units, return_sequences=True))\n",
    "model.add(Dropout(dropout_fraction))\n",
    "#fourth layer\n",
    "model.add(LSTM(units=number_units))\n",
    "model.add(Dropout(dropout_fraction))\n",
    "#output layer\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "# YOUR CODE HERE!\n",
    "model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 1, 50)             10400     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1, 50)             0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 1, 50)             20200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1, 50)             0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 1, 50)             20200     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1, 50)             0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 71,051\n",
      "Trainable params: 71,051\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Summarize the model\n",
    "# YOUR CODE HERE!\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 130 samples, validate on 44 samples\n",
      "Epoch 1/250\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "130/130 [==============================] - 6s 43ms/sample - loss: 0.1364 - val_loss: 0.1032\n",
      "Epoch 2/250\n",
      "130/130 [==============================] - 0s 238us/sample - loss: 0.1297 - val_loss: 0.0971\n",
      "Epoch 3/250\n",
      "130/130 [==============================] - 0s 192us/sample - loss: 0.1237 - val_loss: 0.0911\n",
      "Epoch 4/250\n",
      "130/130 [==============================] - 0s 199us/sample - loss: 0.1170 - val_loss: 0.0850\n",
      "Epoch 5/250\n",
      "130/130 [==============================] - 0s 199us/sample - loss: 0.1105 - val_loss: 0.0789\n",
      "Epoch 6/250\n",
      "130/130 [==============================] - 0s 207us/sample - loss: 0.1039 - val_loss: 0.0726\n",
      "Epoch 7/250\n",
      "130/130 [==============================] - 0s 207us/sample - loss: 0.0971 - val_loss: 0.0663\n",
      "Epoch 8/250\n",
      "130/130 [==============================] - 0s 199us/sample - loss: 0.0890 - val_loss: 0.0599\n",
      "Epoch 9/250\n",
      "130/130 [==============================] - 0s 207us/sample - loss: 0.0803 - val_loss: 0.0534\n",
      "Epoch 10/250\n",
      "130/130 [==============================] - 0s 192us/sample - loss: 0.0739 - val_loss: 0.0471\n",
      "Epoch 11/250\n",
      "130/130 [==============================] - 0s 207us/sample - loss: 0.0687 - val_loss: 0.0411\n",
      "Epoch 12/250\n",
      "130/130 [==============================] - 0s 199us/sample - loss: 0.0626 - val_loss: 0.0356\n",
      "Epoch 13/250\n",
      "130/130 [==============================] - 0s 199us/sample - loss: 0.0546 - val_loss: 0.0309\n",
      "Epoch 14/250\n",
      "130/130 [==============================] - 0s 200us/sample - loss: 0.0515 - val_loss: 0.0268\n",
      "Epoch 15/250\n",
      "130/130 [==============================] - 0s 199us/sample - loss: 0.0427 - val_loss: 0.0237\n",
      "Epoch 16/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0420 - val_loss: 0.0216\n",
      "Epoch 17/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0394 - val_loss: 0.0207\n",
      "Epoch 18/250\n",
      "130/130 [==============================] - 0s 169us/sample - loss: 0.0399 - val_loss: 0.0206\n",
      "Epoch 19/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0395 - val_loss: 0.0210\n",
      "Epoch 20/250\n",
      "130/130 [==============================] - 0s 161us/sample - loss: 0.0426 - val_loss: 0.0212\n",
      "Epoch 21/250\n",
      "130/130 [==============================] - 0s 181us/sample - loss: 0.0329 - val_loss: 0.0212\n",
      "Epoch 22/250\n",
      "130/130 [==============================] - 0s 161us/sample - loss: 0.0408 - val_loss: 0.0210\n",
      "Epoch 23/250\n",
      "130/130 [==============================] - 0s 169us/sample - loss: 0.0410 - val_loss: 0.0206\n",
      "Epoch 24/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0402 - val_loss: 0.0201\n",
      "Epoch 25/250\n",
      "130/130 [==============================] - 0s 166us/sample - loss: 0.0376 - val_loss: 0.0198\n",
      "Epoch 26/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0410 - val_loss: 0.0196\n",
      "Epoch 27/250\n",
      "130/130 [==============================] - 0s 169us/sample - loss: 0.0342 - val_loss: 0.0197\n",
      "Epoch 28/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0434 - val_loss: 0.0198\n",
      "Epoch 29/250\n",
      "130/130 [==============================] - 0s 177us/sample - loss: 0.0385 - val_loss: 0.0197\n",
      "Epoch 30/250\n",
      "130/130 [==============================] - 0s 169us/sample - loss: 0.0364 - val_loss: 0.0194\n",
      "Epoch 31/250\n",
      "130/130 [==============================] - 0s 169us/sample - loss: 0.0392 - val_loss: 0.0191\n",
      "Epoch 32/250\n",
      "130/130 [==============================] - 0s 169us/sample - loss: 0.0399 - val_loss: 0.0187\n",
      "Epoch 33/250\n",
      "130/130 [==============================] - 0s 169us/sample - loss: 0.0385 - val_loss: 0.0184\n",
      "Epoch 34/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0420 - val_loss: 0.0181\n",
      "Epoch 35/250\n",
      "130/130 [==============================] - 0s 169us/sample - loss: 0.0328 - val_loss: 0.0179\n",
      "Epoch 36/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0357 - val_loss: 0.0177\n",
      "Epoch 37/250\n",
      "130/130 [==============================] - 0s 169us/sample - loss: 0.0345 - val_loss: 0.0176\n",
      "Epoch 38/250\n",
      "130/130 [==============================] - 0s 163us/sample - loss: 0.0313 - val_loss: 0.0174\n",
      "Epoch 39/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0317 - val_loss: 0.0170\n",
      "Epoch 40/250\n",
      "130/130 [==============================] - 0s 169us/sample - loss: 0.0359 - val_loss: 0.0164\n",
      "Epoch 41/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0296 - val_loss: 0.0160\n",
      "Epoch 42/250\n",
      "130/130 [==============================] - 0s 169us/sample - loss: 0.0253 - val_loss: 0.0156\n",
      "Epoch 43/250\n",
      "130/130 [==============================] - 0s 169us/sample - loss: 0.0300 - val_loss: 0.0152\n",
      "Epoch 44/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0273 - val_loss: 0.0147\n",
      "Epoch 45/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0329 - val_loss: 0.0142\n",
      "Epoch 46/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0293 - val_loss: 0.0137\n",
      "Epoch 47/250\n",
      "130/130 [==============================] - 0s 199us/sample - loss: 0.0233 - val_loss: 0.0132\n",
      "Epoch 48/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0255 - val_loss: 0.0127\n",
      "Epoch 49/250\n",
      "130/130 [==============================] - 0s 169us/sample - loss: 0.0259 - val_loss: 0.0123\n",
      "Epoch 50/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0246 - val_loss: 0.0118\n",
      "Epoch 51/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0259 - val_loss: 0.0113\n",
      "Epoch 52/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0218 - val_loss: 0.0108\n",
      "Epoch 53/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0221 - val_loss: 0.0103\n",
      "Epoch 54/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0211 - val_loss: 0.0097\n",
      "Epoch 55/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0198 - val_loss: 0.0089\n",
      "Epoch 56/250\n",
      "130/130 [==============================] - 0s 169us/sample - loss: 0.0197 - val_loss: 0.0080\n",
      "Epoch 57/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0217 - val_loss: 0.0072\n",
      "Epoch 58/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0207 - val_loss: 0.0066\n",
      "Epoch 59/250\n",
      "130/130 [==============================] - 0s 215us/sample - loss: 0.0197 - val_loss: 0.0062\n",
      "Epoch 60/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0194 - val_loss: 0.0061\n",
      "Epoch 61/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0148 - val_loss: 0.0064\n",
      "Epoch 62/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0185 - val_loss: 0.0069\n",
      "Epoch 63/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0152 - val_loss: 0.0070\n",
      "Epoch 64/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0169 - val_loss: 0.0065\n",
      "Epoch 65/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0141 - val_loss: 0.0058\n",
      "Epoch 66/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0154 - val_loss: 0.0051\n",
      "Epoch 67/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0169 - val_loss: 0.0049\n",
      "Epoch 68/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0152 - val_loss: 0.0050\n",
      "Epoch 69/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0121 - val_loss: 0.0049\n",
      "Epoch 70/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0218 - val_loss: 0.0051\n",
      "Epoch 71/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0115 - val_loss: 0.0049\n",
      "Epoch 72/250\n",
      "130/130 [==============================] - 0s 192us/sample - loss: 0.0164 - val_loss: 0.0044\n",
      "Epoch 73/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0139 - val_loss: 0.0042\n",
      "Epoch 74/250\n",
      "130/130 [==============================] - 0s 199us/sample - loss: 0.0127 - val_loss: 0.0044\n",
      "Epoch 75/250\n",
      "130/130 [==============================] - 0s 192us/sample - loss: 0.0129 - val_loss: 0.0044\n",
      "Epoch 76/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0187 - val_loss: 0.0050\n",
      "Epoch 77/250\n",
      "130/130 [==============================] - 0s 192us/sample - loss: 0.0126 - val_loss: 0.0053\n",
      "Epoch 78/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0129 - val_loss: 0.0050\n",
      "Epoch 79/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0170 - val_loss: 0.0043\n",
      "Epoch 80/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0138 - val_loss: 0.0038\n",
      "Epoch 81/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0163 - val_loss: 0.0042\n",
      "Epoch 82/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0141 - val_loss: 0.0044\n",
      "Epoch 83/250\n",
      "130/130 [==============================] - 0s 174us/sample - loss: 0.0120 - val_loss: 0.0047\n",
      "Epoch 84/250\n",
      "130/130 [==============================] - 0s 192us/sample - loss: 0.0140 - val_loss: 0.0046\n",
      "Epoch 85/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0110 - val_loss: 0.0044\n",
      "Epoch 86/250\n",
      "130/130 [==============================] - 0s 215us/sample - loss: 0.0173 - val_loss: 0.0043\n",
      "Epoch 87/250\n",
      "130/130 [==============================] - 0s 169us/sample - loss: 0.0154 - val_loss: 0.0042\n",
      "Epoch 88/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0139 - val_loss: 0.0040\n",
      "Epoch 89/250\n",
      "130/130 [==============================] - 0s 169us/sample - loss: 0.0167 - val_loss: 0.0043\n",
      "Epoch 90/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0149 - val_loss: 0.0042\n",
      "Epoch 91/250\n",
      "130/130 [==============================] - 0s 192us/sample - loss: 0.0087 - val_loss: 0.0041\n",
      "Epoch 92/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0095 - val_loss: 0.0039\n",
      "Epoch 93/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0147 - val_loss: 0.0037\n",
      "Epoch 94/250\n",
      "130/130 [==============================] - 0s 192us/sample - loss: 0.0120 - val_loss: 0.0034\n",
      "Epoch 95/250\n",
      "130/130 [==============================] - 0s 169us/sample - loss: 0.0161 - val_loss: 0.0035\n",
      "Epoch 96/250\n",
      "130/130 [==============================] - 0s 185us/sample - loss: 0.0137 - val_loss: 0.0036\n",
      "Epoch 97/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0120 - val_loss: 0.0035\n",
      "Epoch 98/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0123 - val_loss: 0.0034\n",
      "Epoch 99/250\n",
      "130/130 [==============================] - 0s 177us/sample - loss: 0.0125 - val_loss: 0.0034\n",
      "Epoch 100/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0093 - val_loss: 0.0034\n",
      "Epoch 101/250\n",
      "130/130 [==============================] - 0s 215us/sample - loss: 0.0142 - val_loss: 0.0035\n",
      "Epoch 102/250\n",
      "130/130 [==============================] - 0s 230us/sample - loss: 0.0129 - val_loss: 0.0034\n",
      "Epoch 103/250\n",
      "130/130 [==============================] - 0s 192us/sample - loss: 0.0119 - val_loss: 0.0034\n",
      "Epoch 104/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0126 - val_loss: 0.0032\n",
      "Epoch 105/250\n",
      "130/130 [==============================] - 0s 169us/sample - loss: 0.0104 - val_loss: 0.0030\n",
      "Epoch 106/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0084 - val_loss: 0.0029\n",
      "Epoch 107/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0121 - val_loss: 0.0029\n",
      "Epoch 108/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0187 - val_loss: 0.0033\n",
      "Epoch 109/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0121 - val_loss: 0.0039\n",
      "Epoch 110/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0117 - val_loss: 0.0044\n",
      "Epoch 111/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0136 - val_loss: 0.0046\n",
      "Epoch 112/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0139 - val_loss: 0.0046\n",
      "Epoch 113/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0109 - val_loss: 0.0044\n",
      "Epoch 114/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0118 - val_loss: 0.0040\n",
      "Epoch 115/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0120 - val_loss: 0.0038\n",
      "Epoch 116/250\n",
      "130/130 [==============================] - 0s 169us/sample - loss: 0.0123 - val_loss: 0.0037\n",
      "Epoch 117/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0158 - val_loss: 0.0040\n",
      "Epoch 118/250\n",
      "130/130 [==============================] - 0s 192us/sample - loss: 0.0137 - val_loss: 0.0048\n",
      "Epoch 119/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0100 - val_loss: 0.0052\n",
      "Epoch 120/250\n",
      "130/130 [==============================] - 0s 192us/sample - loss: 0.0111 - val_loss: 0.0051\n",
      "Epoch 121/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0141 - val_loss: 0.0046\n",
      "Epoch 122/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0137 - val_loss: 0.0042\n",
      "Epoch 123/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0110 - val_loss: 0.0039\n",
      "Epoch 124/250\n",
      "130/130 [==============================] - 0s 192us/sample - loss: 0.0104 - val_loss: 0.0037\n",
      "Epoch 125/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0134 - val_loss: 0.0038\n",
      "Epoch 126/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0138 - val_loss: 0.0041\n",
      "Epoch 127/250\n",
      "130/130 [==============================] - 0s 169us/sample - loss: 0.0108 - val_loss: 0.0043\n",
      "Epoch 128/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0122 - val_loss: 0.0042\n",
      "Epoch 129/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0113 - val_loss: 0.0041\n",
      "Epoch 130/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0114 - val_loss: 0.0039\n",
      "Epoch 131/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0098 - val_loss: 0.0036\n",
      "Epoch 132/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0105 - val_loss: 0.0034\n",
      "Epoch 133/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0092 - val_loss: 0.0032\n",
      "Epoch 134/250\n",
      "130/130 [==============================] - 0s 169us/sample - loss: 0.0117 - val_loss: 0.0031\n",
      "Epoch 135/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0096 - val_loss: 0.0031\n",
      "Epoch 136/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0142 - val_loss: 0.0032\n",
      "Epoch 137/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0109 - val_loss: 0.0033\n",
      "Epoch 138/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0122 - val_loss: 0.0033\n",
      "Epoch 139/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0094 - val_loss: 0.0033\n",
      "Epoch 140/250\n",
      "130/130 [==============================] - 0s 192us/sample - loss: 0.0103 - val_loss: 0.0030\n",
      "Epoch 141/250\n",
      "130/130 [==============================] - 0s 181us/sample - loss: 0.0135 - val_loss: 0.0030\n",
      "Epoch 142/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0127 - val_loss: 0.0032\n",
      "Epoch 143/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0094 - val_loss: 0.0035\n",
      "Epoch 144/250\n",
      "130/130 [==============================] - 0s 199us/sample - loss: 0.0115 - val_loss: 0.0035\n",
      "Epoch 145/250\n",
      "130/130 [==============================] - 0s 222us/sample - loss: 0.0104 - val_loss: 0.0034\n",
      "Epoch 146/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0104 - val_loss: 0.0033\n",
      "Epoch 147/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0114 - val_loss: 0.0034\n",
      "Epoch 148/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0082 - val_loss: 0.0035\n",
      "Epoch 149/250\n",
      "130/130 [==============================] - 0s 169us/sample - loss: 0.0104 - val_loss: 0.0035\n",
      "Epoch 150/250\n",
      "130/130 [==============================] - 0s 192us/sample - loss: 0.0122 - val_loss: 0.0033\n",
      "Epoch 151/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0098 - val_loss: 0.0031\n",
      "Epoch 152/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0100 - val_loss: 0.0031\n",
      "Epoch 153/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0118 - val_loss: 0.0031\n",
      "Epoch 154/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0117 - val_loss: 0.0031\n",
      "Epoch 155/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0109 - val_loss: 0.0032\n",
      "Epoch 156/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0091 - val_loss: 0.0033\n",
      "Epoch 157/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0093 - val_loss: 0.0036\n",
      "Epoch 158/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0130 - val_loss: 0.0038\n",
      "Epoch 159/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0088 - val_loss: 0.0044\n",
      "Epoch 160/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0123 - val_loss: 0.0049\n",
      "Epoch 161/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0110 - val_loss: 0.0050\n",
      "Epoch 162/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0124 - val_loss: 0.0049\n",
      "Epoch 163/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0115 - val_loss: 0.0047\n",
      "Epoch 164/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0116 - val_loss: 0.0042\n",
      "Epoch 165/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0122 - val_loss: 0.0039\n",
      "Epoch 166/250\n",
      "130/130 [==============================] - 0s 192us/sample - loss: 0.0115 - val_loss: 0.0038\n",
      "Epoch 167/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0097 - val_loss: 0.0037\n",
      "Epoch 168/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0125 - val_loss: 0.0037\n",
      "Epoch 169/250\n",
      "130/130 [==============================] - 0s 169us/sample - loss: 0.0106 - val_loss: 0.0036\n",
      "Epoch 170/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0128 - val_loss: 0.0037\n",
      "Epoch 171/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0092 - val_loss: 0.0039\n",
      "Epoch 172/250\n",
      "130/130 [==============================] - 0s 169us/sample - loss: 0.0092 - val_loss: 0.0040\n",
      "Epoch 173/250\n",
      "130/130 [==============================] - 0s 192us/sample - loss: 0.0100 - val_loss: 0.0039\n",
      "Epoch 174/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0108 - val_loss: 0.0037\n",
      "Epoch 175/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0113 - val_loss: 0.0035\n",
      "Epoch 176/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0111 - val_loss: 0.0034\n",
      "Epoch 177/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0112 - val_loss: 0.0033\n",
      "Epoch 178/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0099 - val_loss: 0.0032\n",
      "Epoch 179/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0112 - val_loss: 0.0031\n",
      "Epoch 180/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0107 - val_loss: 0.0030\n",
      "Epoch 181/250\n",
      "130/130 [==============================] - 0s 199us/sample - loss: 0.0150 - val_loss: 0.0032\n",
      "Epoch 182/250\n",
      "130/130 [==============================] - 0s 169us/sample - loss: 0.0121 - val_loss: 0.0038\n",
      "Epoch 183/250\n",
      "130/130 [==============================] - 0s 199us/sample - loss: 0.0095 - val_loss: 0.0044\n",
      "Epoch 184/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0105 - val_loss: 0.0046\n",
      "Epoch 185/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0104 - val_loss: 0.0044\n",
      "Epoch 186/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0125 - val_loss: 0.0041\n",
      "Epoch 187/250\n",
      "130/130 [==============================] - 0s 207us/sample - loss: 0.0113 - val_loss: 0.0041\n",
      "Epoch 188/250\n",
      "130/130 [==============================] - 0s 222us/sample - loss: 0.0117 - val_loss: 0.0038\n",
      "Epoch 189/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0110 - val_loss: 0.0037\n",
      "Epoch 190/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0097 - val_loss: 0.0035\n",
      "Epoch 191/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0100 - val_loss: 0.0035\n",
      "Epoch 192/250\n",
      "130/130 [==============================] - 0s 169us/sample - loss: 0.0088 - val_loss: 0.0034\n",
      "Epoch 193/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0119 - val_loss: 0.0033\n",
      "Epoch 194/250\n",
      "130/130 [==============================] - 0s 169us/sample - loss: 0.0104 - val_loss: 0.0032\n",
      "Epoch 195/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0087 - val_loss: 0.0032\n",
      "Epoch 196/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0113 - val_loss: 0.0032\n",
      "Epoch 197/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0081 - val_loss: 0.0031\n",
      "Epoch 198/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0105 - val_loss: 0.0030\n",
      "Epoch 199/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0098 - val_loss: 0.0030\n",
      "Epoch 200/250\n",
      "130/130 [==============================] - 0s 186us/sample - loss: 0.0122 - val_loss: 0.0030\n",
      "Epoch 201/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0107 - val_loss: 0.0030\n",
      "Epoch 202/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0113 - val_loss: 0.0030\n",
      "Epoch 203/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0117 - val_loss: 0.0030\n",
      "Epoch 204/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0107 - val_loss: 0.0032\n",
      "Epoch 205/250\n",
      "130/130 [==============================] - 0s 215us/sample - loss: 0.0094 - val_loss: 0.0033\n",
      "Epoch 206/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0109 - val_loss: 0.0033\n",
      "Epoch 207/250\n",
      "130/130 [==============================] - 0s 192us/sample - loss: 0.0073 - val_loss: 0.0032\n",
      "Epoch 208/250\n",
      "130/130 [==============================] - 0s 207us/sample - loss: 0.0142 - val_loss: 0.0032\n",
      "Epoch 209/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0090 - val_loss: 0.0032\n",
      "Epoch 210/250\n",
      "130/130 [==============================] - 0s 199us/sample - loss: 0.0091 - val_loss: 0.0033\n",
      "Epoch 211/250\n",
      "130/130 [==============================] - 0s 192us/sample - loss: 0.0102 - val_loss: 0.0033\n",
      "Epoch 212/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0091 - val_loss: 0.0032\n",
      "Epoch 213/250\n",
      "130/130 [==============================] - 0s 199us/sample - loss: 0.0097 - val_loss: 0.0030\n",
      "Epoch 214/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0103 - val_loss: 0.0030\n",
      "Epoch 215/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0083 - val_loss: 0.0031\n",
      "Epoch 216/250\n",
      "130/130 [==============================] - 0s 192us/sample - loss: 0.0101 - val_loss: 0.0031\n",
      "Epoch 217/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0092 - val_loss: 0.0032\n",
      "Epoch 218/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0085 - val_loss: 0.0033\n",
      "Epoch 219/250\n",
      "130/130 [==============================] - 0s 169us/sample - loss: 0.0105 - val_loss: 0.0034\n",
      "Epoch 220/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0083 - val_loss: 0.0035\n",
      "Epoch 221/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0137 - val_loss: 0.0037\n",
      "Epoch 222/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0097 - val_loss: 0.0038\n",
      "Epoch 223/250\n",
      "130/130 [==============================] - 0s 169us/sample - loss: 0.0105 - val_loss: 0.0037\n",
      "Epoch 224/250\n",
      "130/130 [==============================] - 0s 199us/sample - loss: 0.0105 - val_loss: 0.0035\n",
      "Epoch 225/250\n",
      "130/130 [==============================] - 0s 215us/sample - loss: 0.0118 - val_loss: 0.0033\n",
      "Epoch 226/250\n",
      "130/130 [==============================] - 0s 207us/sample - loss: 0.0091 - val_loss: 0.0032\n",
      "Epoch 227/250\n",
      "130/130 [==============================] - 0s 199us/sample - loss: 0.0071 - val_loss: 0.0031\n",
      "Epoch 228/250\n",
      "130/130 [==============================] - 0s 207us/sample - loss: 0.0101 - val_loss: 0.0030\n",
      "Epoch 229/250\n",
      "130/130 [==============================] - 0s 253us/sample - loss: 0.0075 - val_loss: 0.0029\n",
      "Epoch 230/250\n",
      "130/130 [==============================] - 0s 222us/sample - loss: 0.0106 - val_loss: 0.0028\n",
      "Epoch 231/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0092 - val_loss: 0.0028\n",
      "Epoch 232/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0083 - val_loss: 0.0027\n",
      "Epoch 233/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0094 - val_loss: 0.0027\n",
      "Epoch 234/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0110 - val_loss: 0.0026\n",
      "Epoch 235/250\n",
      "130/130 [==============================] - 0s 192us/sample - loss: 0.0118 - val_loss: 0.0027\n",
      "Epoch 236/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0071 - val_loss: 0.0028\n",
      "Epoch 237/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0096 - val_loss: 0.0029\n",
      "Epoch 238/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0112 - val_loss: 0.0030\n",
      "Epoch 239/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0110 - val_loss: 0.0032\n",
      "Epoch 240/250\n",
      "130/130 [==============================] - 0s 169us/sample - loss: 0.0093 - val_loss: 0.0034\n",
      "Epoch 241/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0140 - val_loss: 0.0037\n",
      "Epoch 242/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0114 - val_loss: 0.0039\n",
      "Epoch 243/250\n",
      "130/130 [==============================] - 0s 177us/sample - loss: 0.0100 - val_loss: 0.0039\n",
      "Epoch 244/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0094 - val_loss: 0.0038\n",
      "Epoch 245/250\n",
      "130/130 [==============================] - 0s 169us/sample - loss: 0.0105 - val_loss: 0.0037\n",
      "Epoch 246/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0093 - val_loss: 0.0036\n",
      "Epoch 247/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0082 - val_loss: 0.0034\n",
      "Epoch 248/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0095 - val_loss: 0.0033\n",
      "Epoch 249/250\n",
      "130/130 [==============================] - 0s 176us/sample - loss: 0.0075 - val_loss: 0.0031\n",
      "Epoch 250/250\n",
      "130/130 [==============================] - 0s 184us/sample - loss: 0.0089 - val_loss: 0.0030\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2848de47888>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "# Use at least 10 epochs\n",
    "# Do not shuffle the data\n",
    "# Experiement with the batch size, but a smaller batch size is recommended\n",
    "# YOUR CODE HERE!\n",
    "batch_size = 60\n",
    "epochs = 250\n",
    "model.fit(\n",
    "    X_train_rnn,\n",
    "    y_train_rnn,\n",
    "    validation_data=(X_val_rnn, y_val_rnn),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance\n",
    "\n",
    "In this section, you will evaluate the model using the test data. \n",
    "\n",
    "You will need to:\n",
    "1. Evaluate the model using the `X_test` and `y_test` data.\n",
    "2. Use the X_test data to make predictions\n",
    "3. Create a DataFrame of Real (y_test) vs predicted values. \n",
    "4. Plot the Real vs predicted values as a line chart\n",
    "\n",
    "### Hints\n",
    "Remember to apply the `inverse_transform` function to the predicted and y_test values to recover the actual closing prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/1 [========================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 78us/sample - loss: 0.0052\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.004819796959820546"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "# YOUR CODE HERE!\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some predictions\n",
    "# YOUR CODE HERE!\n",
    "predicted = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recover the original prices instead of the scaled version\n",
    "predicted_prices = scaler.inverse_transform(predicted)\n",
    "real_prices = scaler.inverse_transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Real</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>54.30</td>\n",
       "      <td>53.554688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>52.52</td>\n",
       "      <td>53.333061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>51.95</td>\n",
       "      <td>51.778431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>52.33</td>\n",
       "      <td>51.289925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>52.70</td>\n",
       "      <td>51.615067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Real  Predicted\n",
       "0  54.30  53.554688\n",
       "1  52.52  53.333061\n",
       "2  51.95  51.778431\n",
       "3  52.33  51.289925\n",
       "4  52.70  51.615067"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame of Real and Predicted values\n",
    "stock_closing = pd.DataFrame({\n",
    "    \"Real\": real_prices.ravel(),\n",
    "    \"Predicted\": predicted_prices.ravel()\n",
    "})\n",
    "stock_closing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2849ff3c6c8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the real vs predicted values as a line chart\n",
    "# YOUR CODE HERE!\n",
    "stock_closing.plot(title='Close price predictions using LSTM RNN on closing price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
